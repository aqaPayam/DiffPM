{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc093b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:53:28.588134Z",
     "start_time": "2025-05-12T21:53:25.062286Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: Load, Inspect & Plot Time Series\n",
    "\n",
    "# 1) Imports\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "\n",
    "# 2) Define the loader & plotter function\n",
    "def load_time_series(data_dir: str, feature: str) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Reads every CSV in `data_dir` whose filename starts with \"3C\", extracts column `feature`,\n",
    "    plots each series, and returns a list of 1D numpy arrays (one per file), \n",
    "    while printing and plotting detailed stats.\n",
    "    \"\"\"\n",
    "    # only CSVs whose basename starts with \"3C\"\n",
    "    pattern = os.path.join(data_dir, \"3C*.csv\")\n",
    "    paths = glob.glob(pattern)\n",
    "    if not paths:\n",
    "        raise FileNotFoundError(f\"No CSV files starting with '3C' found in {data_dir!r}\")\n",
    "    \n",
    "    series_list: List[np.ndarray] = []\n",
    "    lengths = []\n",
    "    \n",
    "    for p in sorted(paths):\n",
    "        fname = os.path.basename(p)\n",
    "        print(f\"\\n– Processing file: {fname}\")\n",
    "        df = pd.read_csv(p)\n",
    "        \n",
    "        if feature not in df.columns:\n",
    "            print(f\"⚠️  Feature {feature!r} not found → skipping\")\n",
    "            continue\n",
    "        \n",
    "        arr = df[feature].dropna().to_numpy(dtype=np.float32)\n",
    "        series_list.append(arr)\n",
    "        lengths.append(len(arr))\n",
    "        print(f\"✔️  Loaded series length: {len(arr)}\")\n",
    "        \n",
    "        # --- Plot this single time series ---\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.plot(arr, alpha=0.7)\n",
    "        plt.title(f\"{feature!r} from {fname}\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(feature)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if not series_list:\n",
    "        raise ValueError(f\"No series extracted for feature {feature!r}\")\n",
    "    \n",
    "    # Summary statistics over all loaded series\n",
    "    lengths = np.array(lengths)\n",
    "    print(\"\\n=== Summary of Loaded Series ===\")\n",
    "    print(f\"Total series: {len(lengths)}\")\n",
    "    print(f\"Min length:   {lengths.min()}\")\n",
    "    print(f\"Max length:   {lengths.max()}\")\n",
    "    print(f\"Mean length:  {lengths.mean():.1f}\")\n",
    "    print(f\"Std  length:  {lengths.std():.1f}\")\n",
    "    \n",
    "    # Plot histogram of lengths\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.hist(lengths, bins=20, edgecolor=\"black\")\n",
    "    plt.title(\"Distribution of Time Series Lengths\")\n",
    "    plt.xlabel(\"Series Length\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return series_list\n",
    "\n",
    "# 3) Execute the loader on your data directory\n",
    "if __name__ == \"__main__\":\n",
    "    #DATA_DIR     = \"/workspace/BatterySOH/PINN4SOH/data/XJTU data\"\n",
    "    DATA_DIR     = \"Payam\"\n",
    "    FEATURE_NAME = \"CC charge time\"\n",
    "    \n",
    "    print(f\"\\n==> Loading & plotting all '{FEATURE_NAME}' series from '3C*.csv' in {DATA_DIR!r}\")\n",
    "    all_series = load_time_series(DATA_DIR, FEATURE_NAME)\n",
    "    \n",
    "    print(f\"\\n✅ Completed loading and plotting {len(all_series)} series.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86435b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:53:29.175798Z",
     "start_time": "2025-05-12T21:53:28.591820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Build & Inspect the Sliding‐Window Dataset (with sampling plot)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is:\n",
    "      - a window of length `window_size`\n",
    "      - the start index of that window\n",
    "      - the total length of its parent series\n",
    "    \"\"\"\n",
    "    def __init__(self, series_list: List[np.ndarray], window_size: int):\n",
    "        self.series_list = series_list\n",
    "        self.window_size = window_size\n",
    "        self.index_map: List[Tuple[int, int]] = []  # (series_idx, start_idx)\n",
    "        \n",
    "        print(f\"\\n==> Constructing TimeSeriesWindowDataset (window_size={window_size})\")\n",
    "        skipped = 0\n",
    "        for si, series in enumerate(series_list):\n",
    "            L = len(series)\n",
    "            if L < window_size:\n",
    "                print(f\"⚠️  Series #{si} (len={L}) shorter than window_size → skipped\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            n_windows = L - window_size + 1\n",
    "            for start in range(n_windows):\n",
    "                self.index_map.append((si, start))\n",
    "        total_windows = len(self.index_map)\n",
    "        \n",
    "        print(f\"✔️  Mapping complete\")\n",
    "        print(f\"Total series provided: {len(series_list)}\")\n",
    "        print(f\"Series skipped (too short): {skipped}\")\n",
    "        print(f\"Total windows generated: {total_windows}\")\n",
    "        \n",
    "        # Plot distribution of start indices\n",
    "        starts = [start for (_, start) in self.index_map]\n",
    "        plt.figure(figsize=(8, 3))\n",
    "        plt.hist(starts, bins=min(30, max(starts)+1), edgecolor=\"black\")\n",
    "        plt.title(\"Distribution of Window Start Indices\")\n",
    "        plt.xlabel(\"Start Index\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        si, start = self.index_map[idx]\n",
    "        series = self.series_list[si]\n",
    "        window = series[start : start + self.window_size]\n",
    "        item = {\n",
    "            \"window\":     torch.from_numpy(window),                # shape (window_size,)\n",
    "            \"start_idx\":  torch.tensor(start, dtype=torch.long),  \n",
    "            \"series_len\": torch.tensor(len(series), dtype=torch.long)\n",
    "        }\n",
    "        # Detailed print for this sample\n",
    "        #print(f\"→ Sample idx={idx}: series #{si}, start={start}, \"\n",
    "        #      f\"series_len={len(series)}, window_shape={item['window'].shape}\")\n",
    "        return item\n",
    "\n",
    "# -------------------------------\n",
    "# Example instantiation, smoke-test & plotting\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    WINDOW_SIZE = 10\n",
    "\n",
    "    print(f\"\\n==> Instantiating dataset with window_size={WINDOW_SIZE}\")\n",
    "    ds = TimeSeriesWindowDataset(all_series, window_size=WINDOW_SIZE)\n",
    "\n",
    "    print(f\"\\nDataset length (total windows): {len(ds)}\\n\")\n",
    "\n",
    "    # Choose a few sample indices\n",
    "    sample_idxs = [0, len(ds)//2, len(ds)-1]\n",
    "\n",
    "    # Print details and collect windows\n",
    "    windows = []\n",
    "    for test_idx in sample_idxs:\n",
    "        print(f\"\\n--- Fetching sample at flattened index {test_idx} ---\")\n",
    "        item = ds[test_idx]\n",
    "        windows.append((test_idx, item[\"window\"].numpy()))\n",
    "\n",
    "    # Plot those sample windows\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for idx, window in windows:\n",
    "        plt.plot(window, label=f\"window @{idx}\")\n",
    "    plt.title(\"Sample Windows\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6fa65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T21:53:29.203047Z",
     "start_time": "2025-05-12T21:53:29.178018Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3: Cosine Schedule + Sinusoidal Embeddings + Improved 1D UNet\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Cosine schedule\n",
    "def cosine_beta_schedule(T: int, s: float = 0.008):\n",
    "    \"\"\"\n",
    "    Returns torch tensors betas, alphas, alpha_bars of length T\n",
    "    following the cosine schedule from Nichol & Dhariwal (2021).\n",
    "    \"\"\"\n",
    "    steps = T + 1\n",
    "    t = torch.linspace(0, T, steps)\n",
    "    f = torch.cos(((t / T) + s) / (1 + s) * math.pi / 2) ** 2\n",
    "    alpha_bars = f / f[0]\n",
    "    # β_t = 1 - ᾱ_t / ᾱ_{t-1}\n",
    "    betas = 1 - alpha_bars[1:] / alpha_bars[:-1]\n",
    "    betas = torch.clamp(betas, max=0.999)\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    # Quick plot\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(betas.cpu(), label=\"βₜ\")\n",
    "    plt.plot(alpha_bars.cpu(), label=\"ᾱₜ\")\n",
    "    plt.title(\"Cosine β-Schedule\")\n",
    "    plt.xlabel(\"t\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return betas, alphas, alpha_bars\n",
    "\n",
    "# 2) Sinusoidal embedding (from Transformer)\n",
    "def get_sinusoidal_embedding(x: torch.Tensor, dim: int):\n",
    "    \"\"\"\n",
    "    x: LongTensor of shape (B,) or (B,1)\n",
    "    returns: FloatTensor (B, dim)\n",
    "    \"\"\"\n",
    "    if x.dim()==2: x = x.squeeze(-1)\n",
    "    device = x.device\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(-math.log(10000) * torch.arange(0, half, device=device) / half)\n",
    "    args = x.float().unsqueeze(-1) * freqs.unsqueeze(0)  # (B, half)\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    if dim % 2 == 1:  # pad if odd\n",
    "        emb = F.pad(emb, (0,1))\n",
    "    return emb  # (B, dim)\n",
    "\n",
    "# 3) Residual Block with FiLM on time+pos embeddings\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, c: int, emb_dim: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(c, c, 3, padding=1)\n",
    "        self.gn1   = nn.GroupNorm(8, c)\n",
    "        self.conv2 = nn.Conv1d(c, c, 3, padding=1)\n",
    "        self.gn2   = nn.GroupNorm(8, c)\n",
    "        self.film  = nn.Linear(emb_dim, c)  # produce an additive bias\n",
    "\n",
    "    def forward(self, x: torch.Tensor, emb: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, C, W)\n",
    "        emb: (B, emb_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        h = self.gn1(h)\n",
    "        # FiLM bias\n",
    "        bias = self.film(emb).unsqueeze(-1)  # (B, C, 1)\n",
    "        h = h + bias\n",
    "        h = F.silu(h)\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.gn2(h)\n",
    "        return x + h  # residual\n",
    "\n",
    "# 4) Improved U-Net model\n",
    "class ImprovedDiffusionUNet1D(nn.Module):\n",
    "    def __init__(self, window_size: int,\n",
    "                 time_emb_dim: int = 128,\n",
    "                 base_channels: int = 64,\n",
    "                 n_res_blocks: int = 4):\n",
    "        super().__init__()\n",
    "        self.W = window_size\n",
    "        self.base_c = base_channels\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "\n",
    "        # input conv\n",
    "        self.init_conv = nn.Conv1d(1, base_channels, 3, padding=1)\n",
    "\n",
    "        # MLP to process sinusoidal time embeddings\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim*4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim*4, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # MLP to fuse start_idx & series_len embeddings\n",
    "        # we will concat their sin'embs → 2*time_emb_dim\n",
    "        self.cond_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim*2, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # stack of residual blocks\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResBlock1D(base_channels, time_emb_dim) for _ in range(n_res_blocks)\n",
    "        ])\n",
    "\n",
    "        # output conv\n",
    "        self.out_conv = nn.Conv1d(base_channels, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                t: torch.Tensor,\n",
    "                start_idx: torch.Tensor,\n",
    "                series_len: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x: (B, W)\n",
    "        t: LongTensor (B,)\n",
    "        start_idx, series_len: LongTensor (B,)\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        # Prepare input\n",
    "        h = x.unsqueeze(1)  # (B,1,W)\n",
    "        h = self.init_conv(h)  # → (B, C, W)\n",
    "\n",
    "        # time embedding\n",
    "        t_emb = get_sinusoidal_embedding(t, self.time_emb_dim)\n",
    "        t_emb = self.time_mlp(t_emb)  # (B, time_emb_dim)\n",
    "\n",
    "        # cond embedding\n",
    "        se = get_sinusoidal_embedding(start_idx, self.time_emb_dim)\n",
    "        le = get_sinusoidal_embedding(series_len, self.time_emb_dim)\n",
    "        cond = torch.cat([se, le], dim=-1)  # (B, 2*time_emb_dim)\n",
    "        cond_emb = self.cond_mlp(cond)      # (B, time_emb_dim)\n",
    "\n",
    "        # combined\n",
    "        emb = t_emb + cond_emb              # (B, time_emb_dim)\n",
    "\n",
    "        # pass through residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            h = block(h, emb)\n",
    "\n",
    "        out = self.out_conv(F.silu(h))      # (B,1,W)\n",
    "        return out.squeeze(1)               # (B,W)\n",
    "\n",
    "print(\"✅ ImprovedDiffusionUNet1D initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91329b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:01:11.714555Z",
     "start_time": "2025-05-12T21:53:29.211054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Training Loop with the Improved Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "T          = 1000             # number of diffusion timesteps\n",
    "EPOCHS     = 500\n",
    "BATCH_SIZE = 64\n",
    "LR         = 1e-3\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Training on: {DEVICE}\")\n",
    "\n",
    "# 1) Build cosine schedule and move to DEVICE\n",
    "betas_tensor, alphas_tensor, alpha_bars_tensor = cosine_beta_schedule(T)\n",
    "betas_tensor      = betas_tensor.to(DEVICE)\n",
    "alphas_tensor     = alphas_tensor.to(DEVICE)\n",
    "alpha_bars_tensor = alpha_bars_tensor.to(DEVICE)\n",
    "\n",
    "# 2) Dataset & DataLoader\n",
    "dataset = TimeSeriesWindowDataset(all_series, window_size=WINDOW_SIZE)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# 3) Model, optimizer, loss\n",
    "model = ImprovedDiffusionUNet1D(window_size=WINDOW_SIZE).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss_hist = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        x0    = batch[\"window\"].to(DEVICE).float()       # (B, window_size)\n",
    "        start = batch[\"start_idx\"].to(DEVICE)            # (B,)\n",
    "        slen  = batch[\"series_len\"].to(DEVICE)           # (B,)\n",
    "\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, T, (x0.size(0),), device=DEVICE)\n",
    "\n",
    "        # Generate noise\n",
    "        eps = torch.randn_like(x0)\n",
    "\n",
    "        # Forward diffusion q(x_t | x_0)\n",
    "        sqrt_ab   = torch.sqrt(alpha_bars_tensor[t]).unsqueeze(-1)\n",
    "        sqrt_1_ab = torch.sqrt(1 - alpha_bars_tensor[t]).unsqueeze(-1)\n",
    "        xt = sqrt_ab * x0 + sqrt_1_ab * eps\n",
    "\n",
    "        # Predict noise\n",
    "        optimizer.zero_grad()\n",
    "        eps_pred = model(xt, t, start, slen)\n",
    "        loss = criterion(eps_pred, eps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    loss_hist.append(avg_loss)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} — Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Qualitative check every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x0s   = dataset[0][\"window\"].unsqueeze(0).to(DEVICE).float()\n",
    "            st0   = dataset[0][\"start_idx\"].unsqueeze(0).to(DEVICE)\n",
    "            sl0   = dataset[0][\"series_len\"].unsqueeze(0).to(DEVICE)\n",
    "            t_hi  = torch.tensor([T - 1], device=DEVICE)\n",
    "            eps0  = torch.randn_like(x0s)\n",
    "            xt0   = (torch.sqrt(alpha_bars_tensor[t_hi]).unsqueeze(-1) * x0s\n",
    "                    + torch.sqrt(1 - alpha_bars_tensor[t_hi]).unsqueeze(-1) * eps0)\n",
    "            epred = model(xt0, t_hi, st0, sl0)\n",
    "\n",
    "            # One reverse step\n",
    "            ab  = alpha_bars_tensor[t_hi].unsqueeze(-1)\n",
    "            a   = alphas_tensor[t_hi].unsqueeze(-1)\n",
    "            b   = betas_tensor[t_hi].unsqueeze(-1)\n",
    "            pred_x0 = (xt0 - torch.sqrt(1 - ab) * epred) / torch.sqrt(a)\n",
    "            reco = pred_x0.squeeze(0).cpu().numpy()\n",
    "            clean= x0s.squeeze(0).cpu().numpy()\n",
    "\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.plot(clean, label=\"x₀\")\n",
    "        plt.plot(reco, label=\"one‐step reverse\", alpha=0.8)\n",
    "        plt.title(f\"Reverse Check @ Epoch {epoch}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 4) Plot training loss curve\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(loss_hist, marker=\"o\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c75fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:01:15.592882Z",
     "start_time": "2025-05-12T22:01:11.717534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Sampling windows across a full-length series (conditional on start_idx & series_len)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "def sample_windows_conditional(\n",
    "    model,\n",
    "    T: int,\n",
    "    betas: torch.Tensor,\n",
    "    alphas: torch.Tensor,\n",
    "    alpha_bars: torch.Tensor,\n",
    "    window_size: int,\n",
    "    start_idxs: List[int],\n",
    "    series_len: int,\n",
    "    device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reverse-diffuse one window per start index, conditioning on start_idx & series_len.\n",
    "    start_idxs: list of ints of length num_windows\n",
    "    Returns: Tensor of shape (num_windows, window_size) on CPU.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_samples = len(start_idxs)\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.randn(num_samples, window_size, device=device)\n",
    "        start_batch      = torch.tensor(start_idxs, dtype=torch.long, device=device)\n",
    "        series_len_batch = torch.full((num_samples,), series_len, dtype=torch.long, device=device)\n",
    "        for t in reversed(range(T)):\n",
    "            t_batch = torch.full((num_samples,), t, dtype=torch.long, device=device)\n",
    "            eps_pred = model(x_t, t_batch, start_batch, series_len_batch)\n",
    "            a_t    = alphas[t].unsqueeze(-1)\n",
    "            ab_t   = alpha_bars[t].unsqueeze(-1)\n",
    "            b_t    = betas[t].unsqueeze(-1)\n",
    "            x0_pred = (x_t - torch.sqrt(1 - ab_t) * eps_pred) / torch.sqrt(a_t)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x_t)\n",
    "                x_t = torch.sqrt(a_t) * x0_pred + torch.sqrt(b_t) * noise\n",
    "            else:\n",
    "                x_t = x0_pred\n",
    "        return x_t.cpu()\n",
    "\n",
    "# Example usage for a series of length 300:\n",
    "series_len  = 300\n",
    "window_size = WINDOW_SIZE  # e.g. 10\n",
    "num_windows = series_len - window_size   # 290\n",
    "start_idxs  = list(range(num_windows))\n",
    "\n",
    "samples = sample_windows_conditional(\n",
    "    model, T,\n",
    "    betas_tensor, alphas_tensor, alpha_bars_tensor,\n",
    "    window_size, start_idxs, series_len, DEVICE\n",
    ")  # shape: (290, 10)\n",
    "\n",
    "# plot aligned windows\n",
    "plt.figure(figsize=(12,6))\n",
    "for i, w in enumerate(samples.numpy()):\n",
    "    xs = np.arange(i, i+window_size)\n",
    "    plt.plot(xs, w, alpha=0.1)\n",
    "plt.xlim(0, series_len)\n",
    "plt.title(f\"{num_windows} sampled windows (L={series_len}, W={window_size})\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e86a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:02:03.636966Z",
     "start_time": "2025-05-12T22:01:15.595405Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell 6: Seeded Sampling & Reconstruction Using First 10 Real Points\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "# --- Helper: reconstruct from overlapping windows ---\n",
    "def reconstruct_series(samples: np.ndarray, window_size: int, series_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given `samples` with shape (num_windows, window_size),\n",
    "    reconstruct the full series of length `series_len` by averaging overlaps.\n",
    "    \"\"\"\n",
    "    recon = np.zeros(series_len, dtype=float)\n",
    "    counts = np.zeros(series_len, dtype=int)\n",
    "    for start, window in enumerate(samples):\n",
    "        recon[start:start+window_size] += window\n",
    "        counts[start:start+window_size] += 1\n",
    "    counts[counts == 0] = 1\n",
    "    return recon / counts\n",
    "\n",
    "# --- Main loop over all series ---\n",
    "for idx, real_series in enumerate(all_series):\n",
    "    L = len(real_series)\n",
    "    W = WINDOW_SIZE\n",
    "    N = L - W\n",
    "    if N <= 0:\n",
    "        print(f\"Series #{idx} (len={L}) too short—skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nSeries #{idx} (len={L}): sampling {N} windows, seeding first window with real data\")\n",
    "\n",
    "    # 1) Prepare start indices for every window\n",
    "    start_idxs: List[int] = list(range(N))\n",
    "\n",
    "    # 2) Sample windows via reverse diffusion\n",
    "    sampled = sample_windows_conditional(\n",
    "        model, T,\n",
    "        betas_tensor, alphas_tensor, alpha_bars_tensor,\n",
    "        window_size=W,\n",
    "        start_idxs=start_idxs,\n",
    "        series_len=L,\n",
    "        device=DEVICE\n",
    "    ).numpy()  # shape: (N, W)\n",
    "\n",
    "    # 3) Overwrite first window (start=0) with ground-truth data\n",
    "    sampled[0] = real_series[:W]\n",
    "\n",
    "    # 4) Reconstruct the series by averaging overlapping windows\n",
    "    reconstructed = reconstruct_series(sampled, W, L)\n",
    "\n",
    "    # 5) Plot real vs. reconstructed\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(real_series, label=\"Real Series\", linewidth=2)\n",
    "    plt.plot(reconstructed, \"--\", label=\"Reconstructed (seeded)\", linewidth=2)\n",
    "    plt.title(f\"Series #{idx}: Real vs. Reconstructed (Seeded)\")\n",
    "    plt.xlabel(\"Time Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21219d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:02:47.977032Z",
     "start_time": "2025-05-12T22:02:03.640013Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Final Test – Plot All Real Series and Sampled Windows with Uniformly Sampled Lengths\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "# 1) Determine min/max lengths across real series\n",
    "lengths = [len(s) for s in all_series]\n",
    "min_len = max(min(lengths), WINDOW_SIZE)  # ensure at least WINDOW_SIZE\n",
    "max_len = max(lengths)\n",
    "print(f\"Real series lengths range from {min(lengths)} to {max_len}\")\n",
    "print(f\"Sampling synthetic series lengths uniformly in [{min_len}, {max_len}]\")\n",
    "\n",
    "# 2) Prepare the figure\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# 3) Plot all real series in light gray\n",
    "for real in all_series:\n",
    "    xs = np.arange(len(real))\n",
    "    plt.plot(xs, real, color='gray', alpha=0.5, linewidth=1)\n",
    "\n",
    "# 4) For each real series, sample a synthetic length and overlay sampled windows\n",
    "for idx, _ in enumerate(all_series):\n",
    "    # a) Draw a synthetic series length uniformly\n",
    "    L_synth = random.randint(min_len, max_len)\n",
    "\n",
    "    # b) Define start indices at steps of 30\n",
    "    start_idxs: List[int] = list(range(0, L_synth - WINDOW_SIZE + 1, 30))\n",
    "    if not start_idxs:\n",
    "        continue\n",
    "\n",
    "    # c) Sample windows via reverse diffusion conditioned on start & length\n",
    "    windows = sample_windows_conditional(\n",
    "        model, T,\n",
    "        betas_tensor, alphas_tensor, alpha_bars_tensor,\n",
    "        WINDOW_SIZE, start_idxs, L_synth, DEVICE\n",
    "    ).numpy()  # shape (num_windows, WINDOW_SIZE)\n",
    "\n",
    "    # d) Plot each sampled window as a red dashed segment\n",
    "    for start, w in zip(start_idxs, windows):\n",
    "        xs = np.arange(start, start + WINDOW_SIZE)\n",
    "        plt.plot(xs, w, linestyle='--', color='red', alpha=0.7, linewidth=1)\n",
    "\n",
    "# 5) Finalize plot\n",
    "plt.xlim(0, max_len)\n",
    "plt.title(\"Real Series (gray) and Sampled Windows on Synthetic Lengths (red dashed)\")\n",
    "plt.xlabel(\"Time Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6609bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T18:15:59.576280Z",
     "start_time": "2025-05-11T18:15:13.750513Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b544cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d8ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fbe11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (batterySOH)",
   "language": "python",
   "name": "batterysoh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
